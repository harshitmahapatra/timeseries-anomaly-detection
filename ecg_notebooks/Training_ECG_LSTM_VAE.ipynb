{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VAE for Anomaly Detection\n",
    "\n",
    "The goal is to train Variational Autoencoders for anomaly detection.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "ECG Heartbeat Categorization Dataset: https://www.kaggle.com/datasets/shayanfazeli/heartbeat?resource=download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch import Tensor, FloatTensor, nn\n",
    "from pandarallel import pandarallel\n",
    "from typing import Optional\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    Subset,\n",
    "    ConcatDataset,\n",
    ")\n",
    "from torch.nn.utils.rnn import (\n",
    "    pad_sequence,\n",
    "    pack_sequence,\n",
    "    pad_packed_sequence,\n",
    "    pack_padded_sequence,\n",
    ")\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from torchmetrics import Accuracy, ConfusionMatrix, AUROC, ROC\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup for reproducibility\n",
    "SEED_VALUE = 500\n",
    "pl.seed_everything(SEED_VALUE)\n",
    "\n",
    "\n",
    "# Constants\n",
    "WINDOW_LENGTH = timedelta(minutes=10)\n",
    "WINDOW_STRIDE = timedelta(minutes=10)\n",
    "PH_THRESHOLD = 7.05\n",
    "MAX_DATETIME_DIFFERENCE_CTG = timedelta(days=270)\n",
    "MIN_MAX_PARAMS = [0, 240, 0, 127]\n",
    "MINIMUM_VALID_WINDOWS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parallel Apply\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test time series dataset\n",
    "ecg_dataset = Path(\"../ecg/ptbdb_abnormal.csv\")\n",
    "df = pd.read_csv(ecg_dataset, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test time series dataset\n",
    "ecg_dataset_abnormal = Path(\"../ecg/ptbdb_abnormal.csv\")\n",
    "df_abnormal = pd.read_csv(ecg_dataset_abnormal, header=None)\n",
    "df_abnormal.head()\n",
    "ecg_row = df_abnormal.iloc[2]\n",
    "print(ecg_row.tail(1).values[0])\n",
    "ecg_row = ecg_row[:-1]\n",
    "print(ecg_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test time series\n",
    "# Generic function to visualize CTGs\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from datetime import timedelta\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def visualize_ecg(\n",
    "    ecg: List,\n",
    "    show_points: bool = False,\n",
    "):\n",
    "    fig = make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    if show_points:\n",
    "        mode = \"lines+markers\"\n",
    "    else:\n",
    "        mode = \"lines\"\n",
    "\n",
    "    marker = dict(color=\"#FFFFFF\", size=5, line=dict(color=\"black\", width=1))\n",
    "    ecg = go.Scatter(\n",
    "        x=[*range(len(ecg))],\n",
    "        y=ecg,\n",
    "        name=f\"ECG\",\n",
    "        mode=mode,\n",
    "        marker=marker,\n",
    "        line=dict(color=\"red\", width=2),\n",
    "    )\n",
    "    fig.append_trace(ecg, row=1, col=1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ecg = df.sample().values.flatten().tolist()\n",
    "fig = visualize_ecg(random_ecg)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    \"\"\"Class representing the ECG dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: Path):\n",
    "        self.data_dir = data_dir\n",
    "        self.normal_df = pd.read_csv(data_dir / \"ptbdb_normal.csv\", header=None)\n",
    "        self.normal_last_row = self.normal_df.shape[0] - 1\n",
    "        self.abnormal_df = pd.read_csv(data_dir / \"ptbdb_abnormal.csv\", header=None)\n",
    "        self.df = pd.concat([self.normal_df, self.abnormal_df])\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        self.df = self.df.rename(columns={int(f\"{len(df.columns)-1}\"): \"label\"})\n",
    "        self._targets = torch.tensor(self.df[\"label\"].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ecg_row: pd.Series = self.df.iloc[index]\n",
    "        label = torch.tensor([ecg_row.tail(1).values[0]])\n",
    "        ecg_row = ecg_row[:-1]\n",
    "        ecg = ecg_row.values.flatten().tolist()\n",
    "        ecg_tensor = torch.from_numpy(np.array([ecg])).float()\n",
    "        ecg_tensor = ecg_tensor.permute(1, 0)\n",
    "        return ecg_tensor, label\n",
    "\n",
    "    @property\n",
    "    def normal_idx(self) -> Tensor:\n",
    "        return [*range(self.normal_last_row + 1)]\n",
    "\n",
    "    @property\n",
    "    def abnormal_idx(self) -> Tensor:\n",
    "        return [*range(self.normal_last_row + 1, self.df.shape[0])]\n",
    "\n",
    "    @property\n",
    "    def targets(self) -> Tensor:\n",
    "        return self._targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_dataset = ECGDataset(data_dir=Path(\"/home/harshit/ecg-anomaly-detection/ecg/\"))\n",
    "ecg, label = ecg_dataset.__getitem__(0)\n",
    "print(ecg.shape)\n",
    "print(label)\n",
    "visualize_ecg(ecg.cpu().flatten().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding Function\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "\n",
    "\n",
    "class ECGDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Datamodule class to load the CTG dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path = Path(\"../ecg\"),\n",
    "        batch_size: int = 32,\n",
    "        split_seed: int = 50,\n",
    "        num_workers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        self.full_dataset = ECGDataset(\n",
    "            self.hparams.data_dir,\n",
    "        )\n",
    "        self.normal_dataset = Subset(self.full_dataset, self.full_dataset.normal_idx)\n",
    "        self.abnormal_dataset = Subset(\n",
    "            self.full_dataset, self.full_dataset.abnormal_idx\n",
    "        )\n",
    "\n",
    "        non_test_indexes, test_indexes = train_test_split(\n",
    "            np.arange(len(self.normal_dataset)),\n",
    "            test_size=0.20,\n",
    "            random_state=self.hparams.split_seed,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        self.data_test_normal = Subset(self.normal_dataset, test_indexes)\n",
    "        # Extract val samples from remaining\n",
    "        train_indexes, val_indexes = train_test_split(\n",
    "            non_test_indexes,\n",
    "            test_size=0.25,\n",
    "            random_state=self.hparams.split_seed,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        # Train and val datasets just containing normal samples\n",
    "        self.data_train = Subset(self.normal_dataset, train_indexes)\n",
    "\n",
    "        # Single batch experiment\n",
    "        #         random_single_batch_indexes = random.sample(sorted(train_indexes), self.hparams.batch_size)\n",
    "        #         self.random_single_batch = Subset(self.normal_dataset, random_single_batch_indexes)\n",
    "\n",
    "        self.data_val = Subset(self.normal_dataset, val_indexes)\n",
    "\n",
    "        # Test dataset containing just abnormal samples\n",
    "        self.data_test = ConcatDataset([self.data_test_normal, self.abnormal_dataset])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=pad_collate,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=pad_collate,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=pad_collate,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=128, num_layers=2):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # setup LSTM layer\n",
    "        # Input size (N,L,Hin)\n",
    "        # Output size (N,L,Hout)\n",
    "        # Hidden size (num_layers, N, Hout)\n",
    "        # Cell size (num_layers, N, Hout)\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_dim, self.hidden_dim, self.num_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        outputs, (hidden, cell) = self.lstm(input, hidden)\n",
    "        return (hidden, cell)\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=2, num_layers=2):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # setup LSTM layer\n",
    "        # Input size (N,L,Hin)\n",
    "        # Output size (N,L,Hout)\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_dim, self.hidden_dim, self.num_layers, batch_first=True\n",
    "        )\n",
    "        # Output size (N,L,Oout)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        output, (hidden, cell) = self.lstm(input, hidden)\n",
    "        prediction = self.linear(output)\n",
    "        return prediction, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Lightning Model\n",
    "class ECG_LSTM_VAE(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = 32\n",
    "        self.latent_dim = 64\n",
    "        self.num_layers = 2\n",
    "        self.beta = 0.1\n",
    "        self.features = 1\n",
    "\n",
    "        self.lr = 1e-3\n",
    "\n",
    "        self.lstm_enc = LSTMEncoder(\n",
    "            input_dim=self.features,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        self.lstm_dec = LSTMDecoder(\n",
    "            input_dim=self.latent_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=self.features,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(self.hidden_dim * self.num_layers, self.latent_dim)\n",
    "        self.fc_var = nn.Linear(self.hidden_dim * self.num_layers, self.latent_dim)\n",
    "\n",
    "    def reparametize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        noise = torch.randn_like(std)\n",
    "\n",
    "        z = mu + (noise * std)\n",
    "        return z\n",
    "\n",
    "    def calculate_and_log_metrics(self, loss_dict, step_type, batch_size):\n",
    "        self.log(\n",
    "            f\"{step_type}_loss\",\n",
    "            loss_dict[\"loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{step_type}_kld_loss\",\n",
    "            loss_dict[\"kld\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{step_type}_recon_loss\",\n",
    "            loss_dict[\"recon_loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_lens):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Pack x\n",
    "        x_packed = pack_padded_sequence(\n",
    "            x, x_lens, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # encode input space to hidden space\n",
    "        enc_hidden = self.lstm_enc(x_packed)\n",
    "        enc_h = enc_hidden[0].view(batch_size, self.hidden_dim * self.num_layers)\n",
    "\n",
    "        # extract latent variable z(hidden space to latent space)\n",
    "        mean = self.fc_mu(enc_h)\n",
    "        logvar = self.fc_var(enc_h)\n",
    "        z = self.reparametize(mean, logvar)\n",
    "\n",
    "        # decode latent space to input space\n",
    "        z = z.repeat(1, seq_len, 1)\n",
    "        z = z.view(batch_size, seq_len, self.latent_dim)\n",
    "        x_hat, _ = self.lstm_dec(z, enc_hidden)\n",
    "        x_hat = torch.flip(x_hat, dims=(1,))\n",
    "\n",
    "        # x_hat, _ = self.lstm_dec(z)\n",
    "\n",
    "        return mean, logvar, x_hat\n",
    "\n",
    "    def loss_function(self, x_hat, x, mean, logvar, batch_size) -> dict:\n",
    "        # recons_loss = F.mse_loss(x_hat, x)\n",
    "        # kld_loss = torch.mean(\n",
    "        #     -0.5 * torch.sum(1 + logvar - mean**2 - logvar.exp(), dim=1), dim=0\n",
    "        # )\n",
    "\n",
    "        recons_loss = torch.sum(\n",
    "            torch.sum(F.mse_loss(x_hat, x, reduction=\"none\"), dim=(1, 2))\n",
    "        )\n",
    "        kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + logvar - mean**2 - logvar.exp(), dim=1), dim=0\n",
    "        )\n",
    "\n",
    "        # To account for minibatches\n",
    "        kld_weight = x.shape[0] / batch_size\n",
    "\n",
    "        loss = recons_loss + (self.beta * kld_weight * kld_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"recon_loss\": recons_loss,\n",
    "            \"kld\": -kld_loss,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=(self.lr))\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": lr_scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _, x_lens, _ = batch\n",
    "        batch_size = x.shape[0]\n",
    "        mean, logvar, x_hat = self.forward(x, x_lens)\n",
    "        loss_dict = self.loss_function(x_hat, x, mean, logvar, batch_size)\n",
    "        self.calculate_and_log_metrics(loss_dict, \"train\", batch_size)\n",
    "        return loss_dict\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _, x_lens, _ = batch\n",
    "        batch_size = x.shape[0]\n",
    "        mean, logvar, x_hat = self.forward(x, x_lens)\n",
    "        loss_dict = self.loss_function(x_hat, x, mean, logvar, batch_size)\n",
    "        self.calculate_and_log_metrics(loss_dict, \"val\", batch_size)\n",
    "        # Plot and log first signal\n",
    "        if batch_idx == 0:\n",
    "            if self.current_epoch == 0:\n",
    "                self.plot_and_save_signal(\n",
    "                    x, x_lens, f\"val_orig_{self.current_epoch+1}_{self.device.index}\"\n",
    "                )\n",
    "            self.plot_and_save_signal(\n",
    "                x_hat, x_lens, f\"val_recon_{self.current_epoch+1}_{self.device.index}\"\n",
    "            )\n",
    "        return loss_dict\n",
    "\n",
    "    def plot_and_save_signal(self, x: Tensor, x_len: List[int], filename: str):\n",
    "        first_ecg = x[0][: x_len[0], :]\n",
    "        fig = visualize_ecg(first_ecg.cpu().flatten().numpy().tolist())\n",
    "        saved_filename = Path(f\"../tmp/{filename}.png\")\n",
    "        fig.write_image(saved_filename)\n",
    "        self.logger.experiment.log_artifact(self.logger.run_id, saved_filename)\n",
    "        saved_filename.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(datamodule, num_epochs=20):\n",
    "    # Clear out the previous models\n",
    "    models_folder = Path(f\"../models/classification/lstm\")\n",
    "    if models_folder.exists():\n",
    "        shutil.rmtree(models_folder)\n",
    "    models_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    mlflow_runs_folder = Path(\"../mlruns\")\n",
    "    mlflow_runs_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    temp_artifacts_folder = Path(\"../tmp\")\n",
    "    temp_artifacts_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=models_folder,\n",
    "        filename=\"models-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=3,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "\n",
    "    callbacks.append(checkpoint_callback)\n",
    "    # callbacks.append(early_stopping_callback)\n",
    "\n",
    "    mlf_logger = MLFlowLogger(\n",
    "        experiment_name=\"ecg_anomaly_lstm\",\n",
    "        tracking_uri=mlflow_runs_folder.absolute().as_uri(),\n",
    "    )\n",
    "    ctg_lstm_classifier = ECG_LSTM_VAE()\n",
    "    trainer = pl.Trainer(\n",
    "        logger=mlf_logger,\n",
    "        max_epochs=num_epochs,\n",
    "        callbacks=callbacks,\n",
    "        num_sanity_val_steps=0,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        # strategy=\"ddp_notebook\",\n",
    "    )\n",
    "    trainer.fit(model=ctg_lstm_classifier, datamodule=datamodule)\n",
    "    return trainer, ctg_lstm_classifier\n",
    "\n",
    "\n",
    "def test_model(trainer, datamodule):\n",
    "    ctg_lstm_classifier = ECG_LSTM_VAE()\n",
    "    trainer.test(model=ctg_lstm_classifier, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_datamodule = ECGDataModule(\n",
    "    data_dir=Path(\"/home/harshit/ecg-anomaly-detection/ecg/\"),\n",
    "    split_seed=SEED_VALUE,\n",
    "    num_workers=16,\n",
    "    batch_size=32,\n",
    ")\n",
    "ecg_datamodule.prepare_data()\n",
    "ecg_datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try training the model with some hyperparameters.\n",
    "\n",
    "trainer, model = train_model_with_hyperparams(datamodule=ecg_datamodule, num_epochs=100)\n",
    "# test_model(trainer=trainer, datamodule=ctg_lstm_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a series of models\n",
    "from typing import List\n",
    "\n",
    "ecg_model = ECG_LSTM_VAE()\n",
    "model = ECG_LSTM_VAE.load_from_checkpoint(\n",
    "    \"../models/classification/lstm/models-epoch=99-val_loss=107.76.ckpt\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def visualize_long_sample(\n",
    "    samples: List,\n",
    "    predictions: List,\n",
    "    means: List,\n",
    "    sigmas: List,\n",
    "    recon_samples: List,\n",
    "    show_points: bool = False,\n",
    "):\n",
    "    single_ecg_sample, _ = samples[0]\n",
    "\n",
    "    ecgs = [ecg for ecg, _ in samples]\n",
    "    ecgs = [item for sublist in ecgs for item in sublist]\n",
    "    ecgs = [item for sublist in ecgs for item in sublist]\n",
    "\n",
    "    recons = [recons for recons in recon_samples]\n",
    "    recons = [item for sublist in recons for item in sublist]\n",
    "    recons = [item for sublist in recons for item in sublist]\n",
    "\n",
    "    labels = [\n",
    "        np.ones(single_ecg_sample.shape[0]) * label.cpu().numpy()\n",
    "        for _, label in samples\n",
    "    ]\n",
    "    labels = [item for sublist in labels for item in sublist]\n",
    "\n",
    "    preds = [np.ones(single_ecg_sample.shape[0]) * pred for pred in predictions]\n",
    "    preds = [item for sublist in preds for item in sublist]\n",
    "\n",
    "    mus = []\n",
    "    for sample_means in means:\n",
    "        sample_mu = []\n",
    "        for mean in sample_means:\n",
    "            sample_mu.append(np.ones(len(single_ecg_sample.data[0])) * mean)\n",
    "        mus.append(sample_mu)\n",
    "\n",
    "    stds = []\n",
    "    for sample_sigmas in sigmas:\n",
    "        sample_std = []\n",
    "        for sigma in sample_sigmas:\n",
    "            sample_std.append(np.ones(len(single_ecg_sample.data[0])) * sigma)\n",
    "        stds.append(sample_std)\n",
    "\n",
    "    fig = make_subplots(rows=4, cols=1, shared_xaxes=True)\n",
    "\n",
    "    if show_points:\n",
    "        mode = \"lines+markers\"\n",
    "    else:\n",
    "        mode = \"lines\"\n",
    "\n",
    "    marker = dict(color=\"#FFFFFF\", size=5, line=dict(color=\"black\", width=1))\n",
    "    ecg = go.Scatter(\n",
    "        x=[*range(len(ecgs))],\n",
    "        y=ecgs,\n",
    "        name=f\"ECG\",\n",
    "        mode=mode,\n",
    "        marker=marker,\n",
    "        line=dict(color=\"red\", width=2),\n",
    "    )\n",
    "    recon = go.Scatter(\n",
    "        x=[*range(len(ecgs))],\n",
    "        y=recons,\n",
    "        name=f\"Reconstruction\",\n",
    "        mode=mode,\n",
    "        marker=marker,\n",
    "        line=dict(color=\"green\", width=2),\n",
    "    )\n",
    "    pred = go.Scatter(\n",
    "        x=[*range(len(labels))],\n",
    "        y=preds,\n",
    "        name=f\"Prediction\",\n",
    "        mode=mode,\n",
    "        marker=marker,\n",
    "        line=dict(color=\"blue\", width=2),\n",
    "    )\n",
    "    label = go.Scatter(\n",
    "        x=[*range(len(labels))],\n",
    "        y=labels,\n",
    "        name=f\"Label\",\n",
    "        mode=mode,\n",
    "        marker=marker,\n",
    "        line=dict(color=\"orange\", width=2),\n",
    "    )\n",
    "\n",
    "    fig.append_trace(ecg, row=1, col=1)\n",
    "    fig.append_trace(recon, row=2, col=1)\n",
    "    fig.append_trace(pred, row=3, col=1)\n",
    "    fig.append_trace(label, row=4, col=1)\n",
    "\n",
    "    # for distribution in np.arange(0, 8):\n",
    "    #     dist_mean = [mu[distribution] for mu in mus]\n",
    "    #     dist_mean = [item for sublist in dist_mean for item in sublist]\n",
    "\n",
    "    #     dist_std = [std[distribution] for std in stds]\n",
    "    #     dist_std = [item for sublist in dist_std for item in sublist]\n",
    "\n",
    "    #     dist_mean_upper = [(mean + abs(std)) for mean, std in zip(dist_mean, dist_std)]\n",
    "    #     dist_mean_lower = [(mean - abs(std)) for mean, std in zip(dist_mean, dist_std)]\n",
    "\n",
    "    #     mean_trace = go.Scatter(\n",
    "    #         x=[*range(len(labels))],\n",
    "    #         y=dist_mean,\n",
    "    #         name=f\"Latent_Var_{distribution+1}\",\n",
    "    #         mode=mode,\n",
    "    #         marker=marker,\n",
    "    #         line=dict(color=\"black\", width=2),\n",
    "    #     )\n",
    "    #     mean_trace_upper = go.Scatter(\n",
    "    #         x=[*range(len(labels))],\n",
    "    #         y=dist_mean_upper,\n",
    "    #         name=f\"Latent_Var_{distribution+1}_upper\",\n",
    "    #         mode=mode,\n",
    "    #         marker=marker,\n",
    "    #         line=dict(color=\"firebrick\", width=2, dash=\"dash\"),\n",
    "    #     )\n",
    "    #     mean_trace_lower = go.Scatter(\n",
    "    #         x=[*range(len(labels))],\n",
    "    #         y=dist_mean_lower,\n",
    "    #         name=f\"Latent_Var_{distribution+1}_lower\",\n",
    "    #         mode=mode,\n",
    "    #         marker=marker,\n",
    "    #         line=dict(color=\"darkgreen\", width=2, dash=\"dash\"),\n",
    "    #     )\n",
    "    #     fig.append_trace(mean_trace, row=3 + distribution + 1, col=1)\n",
    "    #     fig.append_trace(mean_trace_upper, row=3 + distribution + 1, col=1)\n",
    "    #     fig.append_trace(mean_trace_lower, row=3 + distribution + 1, col=1)\n",
    "\n",
    "    # fig = fig.update_layout(width=4000, height=2200)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def generate_long_data_sample(\n",
    "    no_of_segments: int = 5, no_of_abnormal_segments: int = 2\n",
    "):\n",
    "    if no_of_abnormal_segments > no_of_segments:\n",
    "        raise ValueError(\n",
    "            \"No of abnormal segments must be less than the total no of segments.\"\n",
    "        )\n",
    "\n",
    "    no_of_normal_segments = no_of_segments - no_of_abnormal_segments\n",
    "\n",
    "    full_dataset = ECGDataset(data_dir=Path(\"/home/harshit/ecg-anomaly-detection/ecg\"))\n",
    "\n",
    "    normal_dataset = Subset(full_dataset, full_dataset.normal_idx)\n",
    "    abnormal_dataset = Subset(full_dataset, full_dataset.abnormal_idx)\n",
    "\n",
    "    normal_indexes = random.sample(\n",
    "        sorted(full_dataset.normal_idx), no_of_normal_segments\n",
    "    )\n",
    "    abnormal_indexes = random.sample(\n",
    "        sorted(full_dataset.abnormal_idx), no_of_abnormal_segments\n",
    "    )\n",
    "\n",
    "    normal_samples = [\n",
    "        full_dataset.__getitem__(normal_index) for normal_index in normal_indexes\n",
    "    ]\n",
    "    abnormal_samples = [\n",
    "        full_dataset.__getitem__(abnormal_index) for abnormal_index in abnormal_indexes\n",
    "    ]\n",
    "\n",
    "    samples = normal_samples + abnormal_samples\n",
    "\n",
    "    random.shuffle(samples)\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_prediction_for_ecg(ecg):\n",
    "    ecg = ecg.view(1, 187, 1)\n",
    "    mean, logvar, x_hat = model(ecg, [187])\n",
    "    loss_dict = model.loss_function(x_hat, ecg, mean, logvar, 1)\n",
    "    sigma = torch.abs(torch.exp(0.5 * logvar))\n",
    "    x_hat = x_hat.view(187, 1)\n",
    "    return loss_dict[\"recon_loss\"], mean, sigma, x_hat\n",
    "\n",
    "\n",
    "samples = generate_long_data_sample()\n",
    "outputs = [get_prediction_for_ecg(sample) for sample, _ in samples]\n",
    "\n",
    "recons = [recon.detach().cpu().numpy() for _, _, _, recon in outputs]\n",
    "\n",
    "predictions = [recon_loss.detach().cpu().numpy() for recon_loss, _, _, _ in outputs]\n",
    "means = [torch.squeeze(mean).detach().cpu().numpy() for _, mean, _, _ in outputs]\n",
    "sigmas = [torch.squeeze(sigma).detach().cpu().numpy() for _, _, sigma, _ in outputs]\n",
    "\n",
    "visualize_long_sample(\n",
    "    samples=samples,\n",
    "    predictions=predictions,\n",
    "    means=means,\n",
    "    sigmas=sigmas,\n",
    "    recon_samples=recons,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a710e520c235fc9a3ea75d114f992a279dedd3d542754d1b02db1fedfa5a3db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
