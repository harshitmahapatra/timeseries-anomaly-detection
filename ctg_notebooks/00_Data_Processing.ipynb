{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import wfdb\n",
    "from wfdb.io import Record\n",
    "from typing import List\n",
    "import shutil\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# Create folder for dataset\n",
    "dataset_path = Path(\"../ctg/dataset/ctgs\")\n",
    "dataset_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metadata file\n",
    "dataset_records = pd.read_csv(\"../ctg/ctu-chb/RECORDS.csv\")\n",
    "dataset_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all outcomes into a single dataframe\n",
    "def read_associated_outcome(row):\n",
    "    record_no: int = int(row[\"record\"])\n",
    "    record_comments: List[str] = wfdb.rdheader(f\"../ctg/ctu-chb/{record_no}\").comments\n",
    "    try:\n",
    "        ph = float(record_comments[2].split(\" \")[-1])\n",
    "        bcef = float(record_comments[3].split(\" \")[-1])\n",
    "        pco2 = float(record_comments[4].split(\" \")[-1])\n",
    "        be = float(record_comments[5].split(\" \")[-1])\n",
    "        apgar1 = int(record_comments[6].split(\" \")[-1])\n",
    "        apgar5 = int(record_comments[7].split(\" \")[-1])\n",
    "        fetus_age_weeks = int(record_comments[16].split(\" \")[-1])\n",
    "        fetus_weight_grams = float(record_comments[17].split(\" \")[-1])\n",
    "        fetus_sex = int(record_comments[18].split(\" \")[-1])\n",
    "        mother_age_years = int(record_comments[20].split(\" \")[-1])\n",
    "        mother_gravidity = float(record_comments[21].split(\" \")[-1])\n",
    "        mother_parity = int(record_comments[22].split(\" \")[-1])\n",
    "        mother_diabetes = int(record_comments[23].split(\" \")[-1])\n",
    "        mother_hypertension = int(record_comments[24].split(\" \")[-1])\n",
    "        mother_preeclampsia = int(record_comments[25].split(\" \")[-1])\n",
    "        mother_praecox = int(record_comments[26].split(\" \")[-1])\n",
    "        mother_pyrexia = int(record_comments[27].split(\" \")[-1])\n",
    "        mother_meconim = int(record_comments[28].split(\" \")[-1])\n",
    "    except BaseException as e:\n",
    "        print(record_no)\n",
    "        print(e)\n",
    "        raise ValueError()\n",
    "\n",
    "    return {\n",
    "        \"record_no\": record_no,\n",
    "        \"ph\": ph,\n",
    "        \"bcef\": bcef,\n",
    "        \"pco2\": pco2,\n",
    "        \"be\": be,\n",
    "        \"apgar1\": apgar1,\n",
    "        \"apgar5\": apgar5,\n",
    "        \"fetus_age_weeks\": fetus_age_weeks,\n",
    "        \"fetus_weight_grams\": fetus_weight_grams,\n",
    "        \"fetus_sex\": fetus_sex,\n",
    "        \"mother_age_years\": mother_age_years,\n",
    "        \"mother_gravidity\": mother_gravidity,\n",
    "        \"mother_parity\": mother_parity,\n",
    "        \"mother_diabetes\": mother_diabetes,\n",
    "        \"mother_hypertension\": mother_hypertension,\n",
    "        \"mother_preeclampsia\": mother_preeclampsia,\n",
    "        \"mother_praecox\": mother_praecox,\n",
    "        \"mother_pyrexia\": mother_pyrexia,\n",
    "        \"mother_meconim\": mother_meconim,\n",
    "    }\n",
    "\n",
    "\n",
    "outcome_df = dataset_records.apply(read_associated_outcome, axis=1)\n",
    "outcome_df = pd.DataFrame.from_records(outcome_df.to_list())\n",
    "outcome_df.to_parquet(\"../ctg/dataset/outcome.parquet\")\n",
    "outcome_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each record to parquet\n",
    "def convert_record_to_parquet(row):\n",
    "    record_no: int = int(row[\"record_no\"])\n",
    "    record: Record = wfdb.rdrecord(f\"../ctg/ctu-chb/{record_no}\")\n",
    "    record_df = record.to_dataframe()\n",
    "    record_df = record_df[record_df[\"FHR\"] > 0]\n",
    "    record_df = record_df[record_df[\"FHR\"] < 250]\n",
    "    record_df.to_parquet(f\"../ctg/dataset/ctgs/{record_no}.parquet\")\n",
    "\n",
    "\n",
    "outcome_df.apply(convert_record_to_parquet, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CTG length to outcomes\n",
    "outcome_df = pd.read_parquet(\"../ctg/dataset/outcome.parquet\")\n",
    "outcome_df.head()\n",
    "\n",
    "\n",
    "# Add length of CTG\n",
    "def convert_record_to_parquet(row):\n",
    "    if not \"no_of_points\" in row:\n",
    "        record_no: int = int(row[\"record_no\"])\n",
    "        record_df = pd.read_parquet(f\"../ctg/dataset/ctgs/{record_no}.parquet\")\n",
    "        row[\"no_of_points\"] = record_df.shape[0]\n",
    "    return row\n",
    "\n",
    "\n",
    "outcome_df = outcome_df.apply(convert_record_to_parquet, axis=1)\n",
    "outcome_df.to_parquet(\"../ctg/dataset/outcome.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WINDOW_LENGTH = timedelta(minutes=10)\n",
    "WINDOW_STRIDE = timedelta(minutes=10)\n",
    "PH_THRESHOLD = 7.05\n",
    "MAX_DATETIME_DIFFERENCE_CTG = timedelta(days=270)\n",
    "CALCULATED_Z_SCORE_PARAMS = [\n",
    "    1016201603,\n",
    "    138.06500900606565,\n",
    "    324618849082.29016,\n",
    "    1016201462,\n",
    "    21.75170609426872,\n",
    "    606363058472.4089,\n",
    "]\n",
    "Z_SCORE_OFFSET = 2\n",
    "MIN_MAX_PARAMS = [0, 240, 0, 127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Segments in CTGs\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def find_segments_in_ctg(\n",
    "    ctg: pd.DataFrame, threshold: timedelta = timedelta(seconds=5)\n",
    "):\n",
    "    maskf: List[bool] = (ctg.index.to_series().diff() > threshold).values.tolist()\n",
    "    maskb = maskf[1:]\n",
    "    maskb.append(True)\n",
    "    mask = [(x or y) for (x, y) in zip(maskf, maskb)]\n",
    "    mask[0] = True\n",
    "    borders = ctg[mask].index\n",
    "\n",
    "    keep_borders = []\n",
    "\n",
    "    for idx, border in enumerate(borders):\n",
    "        keep_border = True\n",
    "        if idx == 0:\n",
    "            points_in_prev_segment = 0\n",
    "        points_in_prev_segment = ctg[borders[idx - 1] : border].shape[0] - 2\n",
    "        points_in_next_segment = 0\n",
    "        if idx != len(borders) - 1:\n",
    "            points_in_next_segment = ctg[border : borders[idx + 1]].shape[0] - 2\n",
    "        if (points_in_prev_segment < 1) and (points_in_next_segment < 1):\n",
    "            keep_border = False\n",
    "        keep_borders.append(keep_border)\n",
    "\n",
    "    borders = borders[keep_borders]\n",
    "    return borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add segment information to outcomes\n",
    "import traceback\n",
    "\n",
    "MAX_ALLOWED_GAP_IN_SEGMENT = timedelta(seconds=5)\n",
    "\n",
    "\n",
    "def add_segment_information_to_outcome_file(\n",
    "    max_allowed_gap_in_segment: timedelta = timedelta(minutes=5),\n",
    "):\n",
    "    data_dir = Path(\"../ctg/dataset/\")\n",
    "    outcomes = pd.read_parquet(data_dir / \"outcome.parquet\")\n",
    "    outcomes = outcomes.drop(\n",
    "        [\"no_of_segments\", \"max_segment_length\", \"segments_information\", \"keep\"],\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "\n",
    "    def get_segment_information_for_ctg(row):\n",
    "        try:\n",
    "            identifier: str = int(row[\"record_no\"])\n",
    "            ctg = pd.read_parquet(data_dir / \"ctgs\" / f\"{identifier}.parquet\")\n",
    "            borders = find_segments_in_ctg(\n",
    "                ctg=ctg, threshold=max_allowed_gap_in_segment\n",
    "            )\n",
    "            if len(borders) > 0:\n",
    "                total_segments = int(len(borders) / 2)\n",
    "                lengths = [\n",
    "                    (borders[segment * 2 + 1] - borders[segment * 2])\n",
    "                    for segment in range(0, total_segments)\n",
    "                ]\n",
    "                segments_information = [\n",
    "                    {\n",
    "                        \"start\": borders[segment * 2],\n",
    "                        \"end\": borders[segment * 2 + 1],\n",
    "                        \"length\": lengths[segment],\n",
    "                    }\n",
    "                    for segment in range(0, total_segments)\n",
    "                ]\n",
    "\n",
    "                row[\"no_of_segments\"] = total_segments\n",
    "                row[\"max_segment_length\"] = max(lengths)\n",
    "                row[\"segments_information\"] = {\"segments\": segments_information}\n",
    "                row[\"keep\"] = True\n",
    "            else:\n",
    "                row[\"keep\"] = False\n",
    "            return row\n",
    "        except BaseException as e:\n",
    "            print(identifier)\n",
    "            print(e)\n",
    "            traceback.print_exc()\n",
    "            raise ValueError()\n",
    "\n",
    "    outcomes: pd.DataFrame = outcomes.parallel_apply(\n",
    "        get_segment_information_for_ctg, axis=1\n",
    "    )\n",
    "    outcomes = outcomes[outcomes[\"keep\"]]\n",
    "    outcomes = outcomes.drop([\"keep\"], axis=1)\n",
    "    outcomes.to_parquet(data_dir / \"outcome_segment.parquet\")\n",
    "\n",
    "\n",
    "add_segment_information_to_outcome_file(\n",
    "    max_allowed_gap_in_segment=MAX_ALLOWED_GAP_IN_SEGMENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess CTG\n",
    "# 1. Load the CTG\n",
    "# 2. Extract each segment\n",
    "# 3. Preprocess segment: (Resample, Interpolate, Normalize)\n",
    "# 4. Save segment\n",
    "# 5. Delete Main CTG\n",
    "\n",
    "\n",
    "def preprocess_and_split_ctgs():\n",
    "    data_dir = Path(\"../ctg/dataset/\")\n",
    "    outcome = pd.read_parquet(data_dir / \"outcome_segment.parquet\")\n",
    "    filtered_ctg_dir = data_dir / \"ctgs\"\n",
    "    filtered_ctg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def split_ctg_and_preprocess(row):\n",
    "        segment_info_dict = row[\"segments_information\"]\n",
    "        segments = segment_info_dict[\"segments\"]\n",
    "        identifier: str = int(row[\"record_no\"])\n",
    "        ctg_filepath = data_dir / \"ctgs\" / f\"{identifier}.parquet\"\n",
    "        ctg = pd.read_parquet(ctg_filepath)\n",
    "        filtered_segments = []\n",
    "\n",
    "        rejected_segments = 0\n",
    "        for idx, segment in enumerate(segments):\n",
    "            segment_start = segment[\"start\"]\n",
    "            segment_end = segment[\"end\"]\n",
    "            orig_ctg_segment = ctg[segment_start:segment_end]\n",
    "\n",
    "            # Resample\n",
    "            ctg_segment = orig_ctg_segment.resample(\"0.25S\").agg(\n",
    "                {\"FHR\": np.mean, \"UC\": np.mean}\n",
    "            )\n",
    "            fhr_nan_percentage = ctg_segment[\"FHR\"].isna().sum() / ctg_segment.shape[0]\n",
    "            ctg_segment_length = ctg_segment.index.max() - ctg_segment.index.min()\n",
    "\n",
    "            if ctg_segment_length >= WINDOW_LENGTH:\n",
    "                # Interpolate\n",
    "                ctg_segment[\"FHR\"] = ctg_segment[\"FHR\"].interpolate(method=\"linear\")\n",
    "                ctg_segment[\"UC\"] = ctg_segment[\"UC\"].interpolate(method=\"linear\")\n",
    "\n",
    "                # Normalize\n",
    "                ctg_segment[\"FHR\"] = (ctg_segment[\"FHR\"] - MIN_MAX_PARAMS[0]) / (\n",
    "                    MIN_MAX_PARAMS[1] - MIN_MAX_PARAMS[0]\n",
    "                )\n",
    "                ctg_segment[\"UC\"] = (ctg_segment[\"UC\"] - MIN_MAX_PARAMS[2]) / (\n",
    "                    MIN_MAX_PARAMS[3] - MIN_MAX_PARAMS[2]\n",
    "                )\n",
    "\n",
    "                ctg_segment.to_parquet(\n",
    "                    filtered_ctg_dir / f\"{identifier}_{idx-rejected_segments}.parquet\"\n",
    "                )\n",
    "                filtered_segments.append(segment)\n",
    "            else:\n",
    "                rejected_segments = rejected_segments + 1\n",
    "\n",
    "        ctg_filepath.unlink()\n",
    "\n",
    "        segment_info_dict[\"segments\"] = filtered_segments\n",
    "        row[\"segments_information\"] = segment_info_dict\n",
    "        row[\"preprocessed_no_of_segments\"] = len(filtered_segments)\n",
    "        row[\"orig_no_of_segments\"] = len(segments)\n",
    "        return row\n",
    "\n",
    "    #     random_rows = outcome.sample(n=1000)\n",
    "    #     result = random_rows.parallel_apply(split_ctg_and_preprocess,axis=1)\n",
    "\n",
    "    outcome = outcome.parallel_apply(split_ctg_and_preprocess, axis=1)\n",
    "    outcome.to_parquet(data_dir / \"outcomes_preprocessed.parquet\")\n",
    "\n",
    "\n",
    "preprocess_and_split_ctgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def expand_outcome_normal():\n",
    "    data_dir = Path(\"../ctg/dataset/\")\n",
    "    outcome = pd.read_parquet(data_dir / \"outcomes_preprocessed.parquet\")\n",
    "    print(outcome.shape)\n",
    "\n",
    "    def expand_outcome_row(row):\n",
    "        segment_info_dict = row[\"segments_information\"]\n",
    "        segments = segment_info_dict[\"segments\"]\n",
    "        identifier: str = int(row[\"record_no\"])\n",
    "        fetus_age_weeks: int = row[\"fetus_age_weeks\"]\n",
    "        pH = row[\"ph\"]\n",
    "\n",
    "        # Create list to store all the start and end value for rows\n",
    "        start_rows_list = []\n",
    "        end_rows_list = []\n",
    "        no_of_points_list = []\n",
    "        segment_number_list = []\n",
    "        window_number_list = []\n",
    "\n",
    "        for idx, segment in enumerate(segments):\n",
    "            segment_start = segment[\"start\"]\n",
    "            segment_end = segment[\"end\"]\n",
    "            window_roll = 0\n",
    "            window_start = segment_start\n",
    "            window_end = window_start + WINDOW_LENGTH\n",
    "            ctg_segment = pd.read_parquet(\n",
    "                data_dir / \"ctgs\" / f\"{identifier}_{idx}.parquet\"\n",
    "            )\n",
    "\n",
    "            while window_end <= segment_end:\n",
    "                # Create outcome expanded row\n",
    "                start_rows_list.append(window_start)\n",
    "                end_rows_list.append(window_end)\n",
    "                no_of_points_list.append(ctg_segment[window_start:window_end].shape[0])\n",
    "                segment_number_list.append(idx)\n",
    "                window_number_list.append((window_roll + 1))\n",
    "\n",
    "                # Move window\n",
    "                window_roll = window_roll + 1\n",
    "                window_start = segment_start + (window_roll * WINDOW_STRIDE)\n",
    "                window_end = window_start + WINDOW_LENGTH\n",
    "\n",
    "        id_list = [identifier] * len(start_rows_list)\n",
    "        fetus_age_weeks_list = [fetus_age_weeks] * len(start_rows_list)\n",
    "        ph_list = [pH] * len(start_rows_list)\n",
    "        return {\n",
    "            \"identifier\": id_list,\n",
    "            \"start\": start_rows_list,\n",
    "            \"end\": end_rows_list,\n",
    "            \"fetus_age_weeks\": fetus_age_weeks_list,\n",
    "            \"ns_art_ph\": ph_list,\n",
    "            \"no_of_points\": no_of_points_list,\n",
    "            \"segment_number\": segment_number_list,\n",
    "            \"window_number\": window_number_list,\n",
    "        }\n",
    "\n",
    "    outcome_expanded = outcome.parallel_apply(expand_outcome_row, axis=1)\n",
    "\n",
    "    id_list = []\n",
    "    start_rows_list = []\n",
    "    end_rows_list = []\n",
    "    fetus_age_weeks_list = []\n",
    "    ph_list = []\n",
    "    no_of_points_list = []\n",
    "    segment_number_list = []\n",
    "    window_number_list = []\n",
    "\n",
    "    for result in outcome_expanded:\n",
    "        id_list.extend(result[\"identifier\"])\n",
    "        start_rows_list.extend(result[\"start\"])\n",
    "        end_rows_list.extend(result[\"end\"])\n",
    "        fetus_age_weeks_list.extend(result[\"fetus_age_weeks\"])\n",
    "        ph_list.extend(result[\"ns_art_ph\"])\n",
    "        no_of_points_list.extend(result[\"no_of_points\"])\n",
    "        segment_number_list.extend(result[\"segment_number\"])\n",
    "        window_number_list.extend(result[\"window_number\"])\n",
    "\n",
    "    expanded_dict = {\n",
    "        \"identifier\": id_list,\n",
    "        \"start\": start_rows_list,\n",
    "        \"end\": end_rows_list,\n",
    "        \"fetus_age_weeks\": fetus_age_weeks_list,\n",
    "        \"ns_art_ph\": ph_list,\n",
    "        \"no_of_points\": no_of_points_list,\n",
    "        \"segment_number\": segment_number_list,\n",
    "        \"window_number\": window_number_list,\n",
    "    }\n",
    "\n",
    "    expanded_df = pd.DataFrame.from_dict(expanded_dict)\n",
    "    print(expanded_df.shape)\n",
    "    expanded_df.to_parquet(data_dir / \"outcome_expanded.parquet\")\n",
    "\n",
    "\n",
    "expand_outcome_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read expanded outcome\n",
    "data_dir = Path(\"../ctg/dataset/\")\n",
    "outcome = pd.read_parquet(data_dir / \"outcome_expanded.parquet\")\n",
    "print(outcome.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
