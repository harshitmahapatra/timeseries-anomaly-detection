{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import wfdb\n",
    "from wfdb.io import Record\n",
    "from typing import List\n",
    "import shutil\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# Create folder for dataset\n",
    "dataset_path = Path(\"../ctg/dataset\")\n",
    "dataset_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metadata file\n",
    "dataset_records = pd.read_csv(\"../ctg/ctu-chb/RECORDS.csv\")\n",
    "dataset_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a single outcome file\n",
    "# def convert_to_int(comment_line: str):\n",
    "#     value = comment_line.split(' ')[-1]\n",
    "#     if value != 'NaN':\n",
    "#         return int(value)\n",
    "#     else:\n",
    "#         return float('nan')\n",
    "\n",
    "\n",
    "def read_and_process_outcome(row):\n",
    "    record_no: int = int(row[\"record\"])\n",
    "    record_comments: List[str] = wfdb.rdheader(f\"../ctg/ctu-chb/{record_no}\").comments\n",
    "    try:\n",
    "        ph = float(record_comments[2].split(\" \")[-1])\n",
    "        bcef = float(record_comments[3].split(\" \")[-1])\n",
    "        pco2 = float(record_comments[4].split(\" \")[-1])\n",
    "        be = float(record_comments[5].split(\" \")[-1])\n",
    "        apgar1 = int(record_comments[6].split(\" \")[-1])\n",
    "        apgar5 = int(record_comments[7].split(\" \")[-1])\n",
    "        fetus_age_weeks = int(record_comments[16].split(\" \")[-1])\n",
    "        fetus_weight_grams = float(record_comments[17].split(\" \")[-1])\n",
    "        fetus_sex = int(record_comments[18].split(\" \")[-1])\n",
    "        mother_age_years = int(record_comments[20].split(\" \")[-1])\n",
    "        mother_gravidity = float(record_comments[21].split(\" \")[-1])\n",
    "        mother_parity = int(record_comments[22].split(\" \")[-1])\n",
    "        mother_diabetes = int(record_comments[23].split(\" \")[-1])\n",
    "        mother_hypertension = int(record_comments[24].split(\" \")[-1])\n",
    "        mother_preeclampsia = int(record_comments[25].split(\" \")[-1])\n",
    "        mother_praecox = int(record_comments[26].split(\" \")[-1])\n",
    "        mother_pyrexia = int(record_comments[27].split(\" \")[-1])\n",
    "        mother_meconim = int(record_comments[28].split(\" \")[-1])\n",
    "    except BaseException as e:\n",
    "        print(record_no)\n",
    "        print(e)\n",
    "        raise ValueError()\n",
    "\n",
    "    return {\n",
    "        \"record_no\": record_no,\n",
    "        \"ph\": ph,\n",
    "        \"bcef\": bcef,\n",
    "        \"pco2\": pco2,\n",
    "        \"be\": be,\n",
    "        \"apgar1\": apgar1,\n",
    "        \"apgar5\": apgar5,\n",
    "        \"fetus_age_weeks\": fetus_age_weeks,\n",
    "        \"fetus_weight_grams\": fetus_weight_grams,\n",
    "        \"fetus_sex\": fetus_sex,\n",
    "        \"mother_age_years\": mother_age_years,\n",
    "        \"mother_gravidity\": mother_gravidity,\n",
    "        \"mother_parity\": mother_parity,\n",
    "        \"mother_diabetes\": mother_diabetes,\n",
    "        \"mother_hypertension\": mother_hypertension,\n",
    "        \"mother_preeclampsia\": mother_preeclampsia,\n",
    "        \"mother_praecox\": mother_praecox,\n",
    "        \"mother_pyrexia\": mother_pyrexia,\n",
    "        \"mother_meconim\": mother_meconim,\n",
    "    }\n",
    "\n",
    "\n",
    "outcome_df = dataset_records.apply(read_and_process_outcome, axis=1)\n",
    "outcome_df = pd.DataFrame.from_records(outcome_df.to_list())\n",
    "outcome_df.to_parquet(\"../ctg/dataset/outcome.parquet\")\n",
    "outcome_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each record to parquet\n",
    "def convert_record_to_parquet(row):\n",
    "    record_no: int = int(row[\"record_no\"])\n",
    "    record: Record = wfdb.rdrecord(f\"../ctg/ctu-chb/{record_no}\")\n",
    "    record_df = record.to_dataframe()\n",
    "    record_df.to_parquet(f\"../ctg/dataset/{record_no}.parquet\")\n",
    "\n",
    "\n",
    "outcome_df.apply(convert_record_to_parquet, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete wfdb db\n",
    "def delete_unused_items(dir: Path):\n",
    "    for entry in dir.iterdir():\n",
    "        if entry.is_dir():\n",
    "            shutil.rmtree(entry)\n",
    "        elif entry.is_file():\n",
    "            entry.unlink()\n",
    "    dir.rmdir()\n",
    "\n",
    "\n",
    "wfdb_path = Path(\"../ctg/ctu-chb/\")\n",
    "delete_unused_items(wfdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CTG length to outcomes\n",
    "outcome_df = pd.read_parquet(\"../ctg/dataset/outcome.parquet\")\n",
    "outcome_df.head()\n",
    "\n",
    "\n",
    "# Add length of CTG\n",
    "def convert_record_to_parquet(row):\n",
    "    if not \"no_of_points\" in row:\n",
    "        record_no: int = int(row[\"record_no\"])\n",
    "        record_df = pd.read_parquet(f\"../ctg/dataset/{record_no}.parquet\")\n",
    "        row[\"no_of_points\"] = record_df.shape[0]\n",
    "    return row\n",
    "\n",
    "\n",
    "outcome_df = outcome_df.apply(convert_record_to_parquet, axis=1)\n",
    "outcome_df.to_parquet(\"../ctg/dataset/outcome.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(outcome_df, x=\"no_of_points\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand outcome\n",
    "outcome_df = pd.read_parquet(\"../ctg/dataset/outcome.parquet\")\n",
    "outcome_df.head()\n",
    "\n",
    "WINDOW_LENGTH = timedelta(minutes=10)\n",
    "WINDOW_STRIDE = timedelta(minutes=10)\n",
    "\n",
    "\n",
    "def expand_outcome_normal():\n",
    "    data_dir = Path(\"../ctg-data/final-ctg-dataset\")\n",
    "    outcome = pd.read_parquet(data_dir / \"outcomes_preprocessed.parquet\")\n",
    "    print(outcome.shape)\n",
    "\n",
    "    def expand_outcome_row(row):\n",
    "        segment_info_dict = row[\"segments_information\"]\n",
    "        segments = segment_info_dict[\"segments\"]\n",
    "        identifier: str = row[\"ID\"]\n",
    "        datetime_of_birth: datetime = row[\"datetime_of_birth\"]\n",
    "        pH = row[\"ns_art_ph\"]\n",
    "\n",
    "        # Create list to store all the start and end value for rows\n",
    "        start_rows_list = []\n",
    "        end_rows_list = []\n",
    "        no_of_points_list = []\n",
    "        segment_number_list = []\n",
    "        window_number_list = []\n",
    "\n",
    "        for idx, segment in enumerate(segments):\n",
    "            segment_start = segment[\"start\"]\n",
    "            segment_end = segment[\"end\"]\n",
    "            window_roll = 0\n",
    "            window_start = segment_start\n",
    "            window_end = window_start + WINDOW_LENGTH\n",
    "            ctg_segment = pd.read_parquet(\n",
    "                data_dir / \"ctgs\" / \"filtered_segments\" / f\"{identifier}_{idx}.parquet\"\n",
    "            )\n",
    "\n",
    "            while window_end <= segment_end:\n",
    "                # Create outcome expanded row\n",
    "                start_rows_list.append(window_start)\n",
    "                end_rows_list.append(window_end)\n",
    "                no_of_points_list.append(ctg_segment[window_start:window_end].shape[0])\n",
    "                segment_number_list.append(idx)\n",
    "                window_number_list.append((window_roll + 1))\n",
    "\n",
    "                # Move window\n",
    "                window_roll = window_roll + 1\n",
    "                window_start = segment_start + (window_roll * WINDOW_STRIDE)\n",
    "                window_end = window_start + WINDOW_LENGTH\n",
    "\n",
    "        id_list = [identifier] * len(start_rows_list)\n",
    "        dob_list = [datetime_of_birth] * len(start_rows_list)\n",
    "        ph_list = [pH] * len(start_rows_list)\n",
    "        return {\n",
    "            \"identifier\": id_list,\n",
    "            \"start\": start_rows_list,\n",
    "            \"end\": end_rows_list,\n",
    "            \"datetime_of_birth\": dob_list,\n",
    "            \"ns_art_ph\": ph_list,\n",
    "            \"no_of_points\": no_of_points_list,\n",
    "            \"segment_number\": segment_number_list,\n",
    "            \"window_number\": window_number_list,\n",
    "        }\n",
    "\n",
    "    outcome_expanded = outcome.parallel_apply(expand_outcome_row, axis=1)\n",
    "\n",
    "    id_list = []\n",
    "    start_rows_list = []\n",
    "    end_rows_list = []\n",
    "    dob_list = []\n",
    "    ph_list = []\n",
    "    no_of_points_list = []\n",
    "    segment_number_list = []\n",
    "    window_number_list = []\n",
    "\n",
    "    for result in outcome_expanded:\n",
    "        id_list.extend(result[\"identifier\"])\n",
    "        start_rows_list.extend(result[\"start\"])\n",
    "        end_rows_list.extend(result[\"end\"])\n",
    "        dob_list.extend(result[\"datetime_of_birth\"])\n",
    "        ph_list.extend(result[\"ns_art_ph\"])\n",
    "        no_of_points_list.extend(result[\"no_of_points\"])\n",
    "        segment_number_list.extend(result[\"segment_number\"])\n",
    "        window_number_list.extend(result[\"window_number\"])\n",
    "\n",
    "    expanded_dict = {\n",
    "        \"identifier\": id_list,\n",
    "        \"start\": start_rows_list,\n",
    "        \"end\": end_rows_list,\n",
    "        \"datetime_of_birth\": dob_list,\n",
    "        \"ns_art_ph\": ph_list,\n",
    "        \"no_of_points\": no_of_points_list,\n",
    "        \"segment_number\": segment_number_list,\n",
    "        \"window_number\": window_number_list,\n",
    "    }\n",
    "\n",
    "    expanded_df = pd.DataFrame.from_dict(expanded_dict)\n",
    "    print(expanded_df.shape)\n",
    "    expanded_df.to_parquet(data_dir / \"outcome_expanded.parquet\")\n",
    "\n",
    "\n",
    "expand_outcome_normal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
